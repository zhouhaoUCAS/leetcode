# -*- coding: utf-8 -*-
"""
Created on Tue Apr 24 14:51:21 2018

@author: zh
"""
#load numerical packages
import pandas as pd
import numpy as np

import matplotlib.pyplot as plt
import seaborn as sns 

#import data
train=pd.read_csv(r'C:\Users\zh\Downloads\House Prices Advanced Regression Techniques/train.csv')
test=pd.read_csv(r'C:\Users\zh\Downloads\House Prices Advanced Regression Techniques/test.csv')
print('训练集维数：',train.shape)
print('测试集维数：',test.shape)#读取时没有header=0所以仅仅默认第一行为列名，所以说shape 为n-1 乘以m，所
#以shape（1459，80）缺失csv文件只有80列但是有1460行 除非加上header=0才确实相等
alldata=pd.concat((train.iloc[:,1:80],test.iloc[:,1:80]),ignore_index=True)
#所以处理的时候特别注意，如果原数据集有弄好的相当于索引标签就要自己不要处理，列已经默认列了，不算进去维度，
#也索引不到。
print('可操作的数据维度：',alldata.shape)

#1.1 view data 统计量 statistic
view=train.describe(include='all').T#查看所有数据，特征较多故倒置,pd.describe表示查看基础统计量
view['null']=len(train)-view['count']#增加一行各特征缺失值个数
view.insert(0,'dytype',train.dtypes)#插入一行各特征取值类型,dataframe.insert(索引，列名，series等)
view.T.to_csv('view1.csv')#再转置写入CSV文件

view=alldata.describe(include='all').T
view['null']=len(alldata)-view['count']
view.insert(0,'dytype',alldata.dtypes)
view.T.to_csv('view2.csv')

#1.2  相关性分析 correlation analysis
corrmat=train.corr()#创造一个画布和一组子图，相关系数矩阵
f,ax=plt.subplots(figsize=(12,9))#figsize 每英寸高度和宽度，plt.subplots(a,b)生成a行b列子图默认为1，返回二值型元祖，第一个元素为画布，第二为子图a[1][1]第二行第二列
sns.heatmap(train.corr(),ax=ax,vmax=1.0,vmin=0)#sns.heatmap(data,ax=ax[1](指定子图)，vmax,vmin根据data取值)
plt.title('Correlation of features')#plt.title(str)增加标题
ax.set_xlabel('Feature')#ax.set_x(y)label(str)设置xy轴名字  plot()函数是个2D曲线绘图函数
ax.set_ylabel('Feature')#plt.xlabel(str) plt.ylabel(str)

#1.3查看影响价格最重要的几个特征
k=10 #可查看K个和价格相关性最强的变量
plt.figure(figsize=(12,9))#创造一个画布
cols=corrmat.nlargest(k,'SalePrice').index#dataframe.nlargest(k,标签）返回前K个最大的行，nsmallest相反
cm=np.corrcoef(train[cols].values.T)#np.corrcoef 返回协方差矩阵,pd.values按行提取所以转置
sns.set(font_scale=1.5)#设置标签字体大小
hm=sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)
#annot=True显示热力图上数值大小，square=True将图变成正方形默认矩形，fmt字符，xticklabels: 如果是True，则绘制dataframe的列名。如果是False，则不绘制列名。如果是列表，则绘制列表中的内容作为xticklabels。 如果是整数n，则绘制列名，但每个n绘制一个label。 默认为True
#annot_kws，当annot为True时，可设置各个参数，包括大小，颜色，加粗，斜体字等。cbar:是否在热力图侧边绘制颜色刻度条，默认值是True
plt.savefig('相关性强.jpg')#储存图片
plt.show()


corr=train.corr()
corr[corr['SalePrice']>0.5]

#绘制散点矩阵图
sns.set()
cols = ['SalePrice', 'OverallQual', 'GrLivArea', 'GarageCars', 'TotalBsmtSF', 'FullBath', 'YearBuilt']
sns.pairplot(train[cols], size = 2.5)#sns.pairplot(data)绘制散点图矩阵，多变量数据不能有空值，不然会出错
plt.show()

#查看价格分布
train['SalePrice'].describe()

#画直方图  质量分布图
from scipy import stats  #引入统计包
from scipy.stats import norm #引入正态函数
from matplotlib.font_manager import FontProperties #引入字体属性

font_set = FontProperties(fname=r"c:\windows\fonts\simsun.ttc", size=12)#设置字体位置，fname=表明文件位置 

sns.distplot(train['SalePrice'],fit=norm)#sns.distplot画直方图，fit拟合函数
plt.title(u'直方图',fontproperties=font_set)
fig=plt.figure()
stats.probplot(train['SalePrice'],plot=plt)#理论分布的分位数（默认情况下的正态分布）生成样本数据的概率图。 probplot可选地计算数据的最佳拟合线，并使用Matplotlib或给定的绘图函数绘制结果
#plot=plt表示绘图matplotlib.pyplot模块 ，无不创建绘图
plt.title(u'PP图',fontproperties=font_set)#设置字体


import math
print('Skewness:%f'%train['SalePrice'].skew())#偏度 负左边尾巴长  正右边尾巴长
print('kurtosis:%f'%train['SalePrice'].kurt())#峰度

# 研究SalePrice和GrLivArea的关系
data1 = pd.concat([train['SalePrice'], train['GrLivArea']], axis=1)
data1.plot.scatter(x='GrLivArea', y='SalePrice', ylim=(0,800000));#绘制散点图data.plot.scatter(x,y)
plt.title(u'SalePrice和GrLivArea的散点图',fontproperties=font_set) #加u就是告诉python后面的是个unicode编码
# 直方图和正态概率图，查看是否正态分布
fig = plt.figure()
sns.distplot(train['GrLivArea'], fit=norm);
plt.title(u'直方图',fontproperties=font_set)
fig = plt.figure()
res = stats.probplot(train['GrLivArea'], plot=plt)
plt.title(u'分布图',fontproperties=font_set)
##  由散点图可知，图像的右下角存在两个异常值，建议去除；图像非正态分布



# 研究SalePrice和OverallQual的关系
data2 = pd.concat([train['SalePrice'], train['OverallQual']], axis=1)
f, ax = plt.subplots(figsize=(8, 6))
fig = sns.boxplot(x='OverallQual', y="SalePrice", data=data2)#设置 x,y 以及颜色控制的变量,data为输入的数据集
fig.axis(ymin=0, ymax=800000);#plt.axis([xmin, xmax, ymin, ymax])给定坐标轴范围，xlim(xmin, xmax)和ylim(ymin, ymax)来调整x,y坐标范围


#每月销售额
a=train.groupby('MoSold')['SalePrice'].count()
print(train.groupby('MoSold')['SalePrice'].count())#train.groupby('MoSold')等价于groupby(train['MoSold]),统计每月销售量

sns.countplot(x='MoSold',data=train)#x输入列标签可解释实例，计数图，分类变量直方图。
plt.show()


#数据集合并之后，我们就可以对整个数据集进行处理了。首先我们要注意变量的类型。比如，MSSubClass是一个
#类别型的变量而非数值型变量，尽管它是由数字来表示的。pandas会将这类数字符号当成数字处理。这样会使我们
#后面的模型训练不准确，所以我们要把它变回string类型。



#基础清理
#查看缺失值情况
def missing_values (alldata):
    alldata_na=pd.DataFrame(alldata.isnull().sum(),columns={'missingnum'})#false当作0必须用sum()而不是count()
    alldata_na['missingRatio']=alldata_na['missingnum']/len(alldata)#len()获得行数
    alldata_na['existnum']=len(alldata)-alldata_na['missingnum']
    
    alldata_na['train_notna']=len(train)-train.isnull().sum()
    alldata_na['test_notna']=alldata_na['existnum']-alldata_na['train_notna']
    alldata_na['dtypes']=alldata.dtypes#dtypes返回series，如果一列多个类型或字符串类型返回OBJECT

    alldata_na=alldata_na[alldata_na['missingnum']>0].reset_index().sort_values(by=['missingnum','index'],ascending=[False,True])
    #dataframe.reset_index()将dataframe变回默认整数索引排成一列列名为index，才可以排序，False降序，方法里面参数输入都是中括号
    alldata_na.set_index('index',inplace=True)#set_index打回标签，by=str或者list(str),inplace直接改变alldata否则改变的副本
    return alldata_na

alldata_na=missing_values(alldata)
alldata_na


#逐个处理缺失数
na_index=alldata_na.index
na_index[0]
alldata[na_index[0]].value_counts()#value_counts()查看频率
#可知poolarea和poolqc有关
print('查看poolarea无但有poolqc的数据：',alldata[(alldata['PoolArea']==0)&(alldata['PoolQC'].notnull())][['PoolArea','PoolQC']])
#dataframe 输入str or list(str)多个就列表 ()&()
print('查看1不为0但2无的数据：',alldata[(alldata['PoolArea']!=0)&(alldata['PoolQC'].isnull())][['PoolArea','PoolQC']])
print('查看1不位0有2有的数据：',alldata[(alldata['PoolArea']!=0)&(alldata['PoolQC'].notnull())][['PoolArea','PoolQC']])
print('查看1为0有2也没的数据：',alldata[(alldata['PoolArea']!=0)&(alldata['PoolQC'].notnull())][['PoolArea','PoolQC']])

print('不同PoolQC的oolArea的均值：',alldata.groupby('PoolQC')['PoolArea'].mean())



#处理带有‘garage’前缀的特征
a=pd.Series(alldata.columns)#提取列名到Series
garagelist=a[a.str.contains('Garage')].values#series.values  变成一维array

print('与车库有关：',garagelist)

print(alldata.loc[:,garagelist])    #iloc只能坐标不能字符 loc相反  ix都可以
print(alldata_na.loc[garagelist,:])


print(len(alldata[(alldata['GarageArea']==0) & (alldata['GarageCars']==0)]))# 157
print(len(alldata[(alldata['GarageArea']!=0) & (alldata['GarageCars'].isnull==0)])) # 0为什么isnull()错误

#查看和“Bsmt"有关的特征
a=pd.Series(alldata.columns)#alldata.index 返回标签列表
Bsmt_columns=a[a.str.contains('Bsmt')].values

print('与BSMT有关:',alldata_na.ix[Bsmt_columns,:])
alldata[(alldata['BsmtExposure'].isnull())&alldata['BsmtCond'].notnull()][Bsmt_columns]
#查看BmstExposure为空时的数据
print('查看：',alldata[(alldata['BsmtExposure'].isnull())&(alldata['BsmtCond'].notnull())][Bsmt_columns])


#一并处理
print(alldata['BsmtFinSF1'].value_counts().head(5))# 空值填0
 
print(alldata['BsmtFinSF2'].value_counts().head(5))# 空值填0
 
print(alldata['BsmtFullBath'].value_counts().head(5))# 空值填0
 
print(alldata['BsmtHalfBath'].value_counts().head(5))# 空值填0
 
print(alldata['BsmtFinType1'].value_counts().head(5)) # 空值填众数
 
print(alldata['BsmtFinType2'].value_counts().head(5)) # 空值填众数

#一并处理,直接打印数量方便查阅

print(alldata[['MasVnrType', 'MasVnrArea']].isnull().sum())
 
print(len(alldata[(alldata['MasVnrType'].isnull())& (alldata['MasVnrArea'].isnull())])) # 23
 
print(len(alldata[(alldata['MasVnrType'].isnull())& (alldata['MasVnrArea'].notnull())]))
 
print(len(alldata[(alldata['MasVnrType'].notnull())& (alldata['MasVnrArea'].isnull())]))
 
print(alldata['MasVnrType'].value_counts())
#由于存在Type为空而Area不为空所以按type分组 类别要小于
alldata.groupby('MasVnrType')['MasVnrArea'].median()#即Vnr每个值的Area的均值
alldata[(alldata['MasVnrType'].isnull())& (alldata['MasVnrArea'].notnull())][['MasVnrType','MasVnrArea']]

a=pd.Series([0,1,2,3,None])
a.isnull()

a=pd.Series([0,1,2,3,np.nan])#可读取NA和NaN但无法输入
a.isnull()

df = pd.DataFrame([['ant', 'bee', 'cat'], ['dog', None, 'fly']])
df.isnull()

df = pd.DataFrame([[1, 2, None], [1, None, 3]])
df.isnull()

#读取csv文件里的None是当作字符串的  NA是缺失值
alldata[(alldata['MasVnrType']=='None')&(alldata['MasVnrArea']!=0)][['MasVnrType','MasVnrArea']]
alldata[alldata['MasVnrType']=='None']['MasVnrArea'].value_counts()
#alldata[alldata['MasVnrType']=='None'][['MasVnrArea']].value_counts()错误，value_counts()只能series的方法，而pd[['列名']其实也是1742 rows x 1 columns的dataframe如果再dataframe['']就是Series如列名



print(alldata[alldata['MSSubClass'].isnull() | alldata['MSZoning'].isnull()][['MSSubClass','MSZoning']])
#查看交叉表，只能看非缺失值的，crosstab传入pd列名就好
pd.crosstab(alldata.MSSubClass, alldata.MSZoning)#pd.crosstab(pd.colunmns(index),pd.columns(columns))计算两个（或更多）因子的简单交叉列表。默认情况下，计算因子的频率表，除非传递值数组和聚合函数
#alldta.MSSubClass等价于alldata['MSSubClass']

print(alldata[["LotFrontage", "Neighborhood"]].isnull().sum())
alldata["LotFrontage"].value_counts().head(10)
alldata["Neighborhood"].value_counts().head(10)

#试着填充
print(alldata.groupby('Neighborhood')['LotFrontage'].transform(lambda x: x.fillna(x.median())))

others = ['Functional','Utilities','SaleType','Electrical', "FireplaceQu",'Alley',"Fence", "MiscFeature",\
          'KitchenQual',"LotFrontage",'Exterior1st','Exterior2nd']
print(alldata[others].isnull().sum())
 
print(alldata['Functional'].value_counts().head(5)) # 填众数
print(alldata['Utilities'].value_counts().head(5)) # 填众数
print(alldata['SaleType'].value_counts().head(5)) # 填众数
print(alldata['Electrical'].value_counts().head(5)) # 填众数
print(alldata["Fence"].value_counts()) # 填众数
print(alldata["MiscFeature"].value_counts().head(5)) # 填众数
print(alldata['KitchenQual'].value_counts().head(5)) # 填众数
print(alldata['Exterior1st'].value_counts().head(5)) # 填众数
print(alldata['Exterior2nd'].value_counts().head(5)) # 填众数
print(alldata['FireplaceQu'].value_counts().head(5)) # 填'none'
print(alldata['Alley'].value_counts().head(5)) # 填'none

alldata[alldata[alldata.columns].duplicated()==True]
alldata[alldata.duplicated()]# 但是考虑到当前重复值后来不影响应用 所以可以不用删除
#DataFrame.duplicated（subset = None，keep ='first' ）参数：	subset：列标签或标签序列，可选考虑用于标识重复项的某些列，默认情况下使用所有列，{'first'，'last'，False}，默认'first'first：标记重复，True除了第一次出现。last：标记重复，True除了最后一次出现。错误：将所有重复项标记为True。
# 判断是否重复
#data.duplicated()`，是为True，默认仅第一个为False
#pd.duplicated()返回bool型Series
#移除重复数据
#data.drop_duplicated()

#对指定列判断是否存在重复值，然后删除重复数据
#data.drop_duplicated(['key1'])


#填补缺失值
poolqcna = alldata[(alldata['PoolQC'].isnull())& (alldata['PoolArea']!=0)][['PoolQC','PoolArea']]
areamean = alldata.groupby('PoolQC')['PoolArea'].mean()#因为泳池大小存在就有泳池质量，所以用泳池质量各评级相应泳池大小平均值和带缺失值泳池评级的泳池大小就可以得出相近泳池质量
for i in poolqcna.index:
    v = alldata.ix[i,['PoolArea']].values
    print(type(np.abs(v-areamean)))
    alldata.ix[i,['PoolQC']] = np.abs(v-areamean).astype('float64').argmin()#pd.argmin()最小值和最大值的索引
    
alldata['PoolQC'] = alldata["PoolQC"].fillna("None")
alldata['PoolArea'] = alldata["PoolArea"].fillna(0)


#对于Garage*相关空值  字符型用‘None'填充，数值型用0填充 并且astype转化为字符型可以看频分布（value_counts())
alldata[['GarageCond','GarageFinish','GarageQual','GarageType']] = alldata[['GarageCond','GarageFinish','GarageQual','GarageType']].fillna('None')
alldata[['GarageCars','GarageArea']] = alldata[['GarageCars','GarageArea']].fillna(0)

alldata['Electrical'] = alldata['Electrical'].fillna( alldata['Electrical'].mode()[0])#缺失值少用众数
#如果缺失值是定距型的,就以该属性存在值的平均值来插补缺失的值;如果缺失值是非定距型的,就根据统计学中的众数原理,用该属性的众数(即出现频率最高的值)
#数据划分为三类:定距型数据(Scale)、定序型数据(Ordinal)、定类型数据(Nominal)
# 注意此处'GarageYrBlt'尚未填充
#df=pd.DataFrame({'A':[1,2,1,2,5,6],'B':[1,2,1,1,2,6]})
#df.mode(),mode()默认按列返回众数，多个就多行
#iloc;ix;loc[n]表示取行

#填补地下室
a=pd.Series(alldata.columns)
bsmtcol=a[a.str.contains('Bsmt')].values
#a = pd.Series(alldata.columns) #Series.str.contains(),返回BOLL型series
#BsmtList = a[a.str.contains('Bsmt')].values#values把值返回成array

condition1=(alldata['BsmtExposure'].isnull())&(alldata['BsmtCond'].notnull())#返回的是一个bool型Series
#alldata['BsmtExposure'].fillna(alldata['BsmtExposure'].mode()[0])就没利用条件
alldata.loc[condition1,'BsmtExposure']=alldata['BsmtExposure'].mode()[0]
#type()查看对象类型  ，dir（）查看大多数属性（方法）


condition2 = (alldata['BsmtCond'].isnull()) & (alldata['BsmtExposure'].notnull()) # 3个
alldata.ix[(condition2),'BsmtCond'] = alldata.ix[(condition2),'BsmtQual']
 
condition3 = (alldata['BsmtQual'].isnull()) & (alldata['BsmtExposure'].notnull()) # 2个
alldata.ix[(condition3),'BsmtQual'] = alldata.ix[(condition3),'BsmtCond']

condition4 = (alldata['BsmtFinType1'].notnull()) & (alldata['BsmtFinType2'].isnull())
alldata.ix[condition4,'BsmtFinType2'] = 'Unf'


allBsmtNa = alldata_na.ix[bsmtcol,:]
allBsmtNa_obj = allBsmtNa[allBsmtNa['dtypes']=='object'].index
allBsmtNa_flo = allBsmtNa[allBsmtNa['dtypes']!='object'].index
alldata[allBsmtNa_obj]=alldata[allBsmtNa_obj].fillna('None')
alldata[allBsmtNa_flo]=alldata[allBsmtNa_flo].fillna(0)#区分OBJECT  特征多的



#继续  数值型填none照样是NA
#DataFrame.ix[]多个字符串要变成列表，str   or   list(str)
conditionmas=(alldata['MasVnrType'].isnull())&(alldata['MasVnrArea'].notnull())#返回series
masmean=alldata.groupby('MasVnrType')['MasVnrArea'].median()
for i in alldata.ix[conditionmas,:].index:
    v=alldata.ix[i,['MasVnrArea']].values
    alldata.loc[i,['MasVnrType']]=np.abs(v-masmean).argmin()


#MasVnrM = alldata.groupby('MasVnrType')['MasVnrArea'].median()
#mtypena = alldata[(alldata['MasVnrType'].isnull())& (alldata['MasVnrArea'].notnull())][['MasVnrType','MasVnrArea']]
#for i in mtypena.index:
 #   v = alldata.loc[i,['MasVnrArea']].values
  #  alldata.loc[i,['MasVnrType']] = np.abs(v-MasVnrM).astype('float64').argmin()

alldata['MasVnrType']=alldata['MasVnrType'].fillna('None')
alldata['MasVnrArea']=alldata['MasVnrArea'].fillna(0)



#pd.transform()对元素级操作，lambda x: x.fillna(x.mode()[0] ）    对每个元素填充当列众数
alldata["MSZoning"] = alldata.groupby("MSSubClass")["MSZoning"].transform(lambda x: x.fillna(x.mode()[0]))

#最小二乘法多项式拟合曲线np.polyfit(x,y,deg)deg多项式次数，线性回归就选1

x = alldata.loc[alldata["LotFrontage"].notnull(), "LotArea"]
y = alldata.loc[alldata["LotFrontage"].notnull(), "LotFrontage"]
t = (x <= 25000) & (y <= 150)
p = np.polyfit(x[t], y[t], 1)#生成多项式对象

np.polyval(p, alldata.loc[alldata['LotFrontage'].isnull(), 'LotArea'])
#np.polyval(p,alldata.loc[alldata['LotFrontage'].isnull(),'LotArea'])返回p多项式在横轴alldata.loc[alldata['LotFrontage'].isnull(), 'LotArea']上的值
alldata.ix[alldata['LotFrontage'].isnull(),'LotFrontage']=np.polyval(p, alldata.loc[alldata['LotFrontage'].isnull(), 'LotArea'])
#Series不用中括号


alldata['KitchenQual'] = alldata['KitchenQual'].fillna(alldata['KitchenQual'].mode()[0]) # 用众数填充
alldata['Exterior1st'] = alldata['Exterior1st'].fillna(alldata['Exterior1st'].mode()[0])
alldata['Exterior2nd'] = alldata['Exterior2nd'].fillna(alldata['Exterior2nd'].mode()[0])
alldata["Functional"] = alldata["Functional"].fillna(alldata['Functional'].mode()[0])
alldata["SaleType"] = alldata["SaleType"].fillna(alldata['SaleType'].mode()[0])
alldata["Utilities"] = alldata["Utilities"].fillna(alldata['Utilities'].mode()[0])
 
alldata[["Fence", "MiscFeature"]] = alldata[["Fence", "MiscFeature"]].fillna('None')
alldata['FireplaceQu'] = alldata['FireplaceQu'].fillna('None')
alldata['Alley'] = alldata['Alley'].fillna('None')


#检查多少还有NA值的
alldata.isnull().sum()[alldata.isnull().sum()>0]

#有些数值变量明显不能写众数也不能用0  如年份缺失，可将其离散化然后填None值
#生成series然后利用map将series 或字典相互映射,连续数据离散化
year_map=pd.concat(pd.Series('YearGroup'+str(i+1),index=range(1871+i*20,1891+i*20))for i in range(0,7))
#x for i in range(),pd. concat (pd.Series)

alldata.GarageYrBlt=alldata.GarageYrBlt.map(year_map)
alldata.GarageYrBlt=alldata.GarageYrBlt.fillna('None')


#填补完成
plt.figure(figsize=(8,6))
plt.scatter(train.GrLivArea,train.SalePrice)
plt.show()
 
# # 删除掉异常值GrLivArea>4000但是销售价格低于200000的记录
outliers_id = train[(train.GrLivArea>4000) & (train.SalePrice<200000)].index
print(outliers_id)
alldata=alldata.drop(outliers_id)#丢掉那些行
Y = train.SalePrice.drop(outliers_id)#丢掉Train  Y那异常的值

plt.figure(figsize=(8,6))
plt.scatter(train.GrLivArea,train.SalePrice)
plt.title(u'散点图',fontproperties=font_set)
plt.xlabel('GrLivArea')
plt.ylabel('Price')



train_now=pd.concat([alldata.iloc[:1458,:],Y],axis=1)#默认行添加，pd.concat([df1,df2],axis=0)

test_now = alldata.iloc[1458:,:]


train_now.to_csv('train_afterclean.csv')
test_now.to_csv('test_afterclean.csv')




#读取
train = pd.read_csv("train_afterclean.csv")
test = pd.read_csv("test_afterclean.csv")
alldata = pd.concat((train.loc[:,'MSSubClass':'SaleCondition'], test.loc[:,'MSSubClass':'SaleCondition']), ignore_index=True)
alldata.shape

#处理序列型数据
ordinalmap={'Ex': 5,'Gd': 4,'TA': 3,'Fa': 2,'Po': 1,'None': 0}
ordinalList = ['ExterQual', 'ExterCond', 'GarageQual', 'GarageCond','PoolQC','FireplaceQu', 'KitchenQual', 'HeatingQC', 'BsmtQual','BsmtCond']
#alldata.ix[:,ordinalList]=alldata.ix[:,ordinalList].map(ordinalmap)错误dataframe 没有map方法
for i in ordinalList:
    alldata[i]=alldata[i].map(ordinalmap)
    
#处理类别型数据


alldata['BsmtExposure'] = alldata['BsmtExposure'].map({'None':0, 'No':1, 'Mn':2, 'Av':3, 'Gd':4})    
alldata['BsmtFinType1'] = alldata['BsmtFinType1'].map({'None':0, 'Unf':1, 'LwQ':2,'Rec':3, 'BLQ':4, 'ALQ':5, 'GLQ':6})
alldata['BsmtFinType2'] = alldata['BsmtFinType2'].map({'None':0, 'Unf':1, 'LwQ':2,'Rec':3, 'BLQ':4, 'ALQ':5, 'GLQ':6})
alldata['Functional'] = alldata['Functional'].map({'Maj2':1, 'Sev':2, 'Min2':3, 'Min1':4, 'Maj1':5, 'Mod':6, 'Typ':7})
alldata['GarageFinish'] = alldata['GarageFinish'].map({'None':0, 'Unf':1, 'RFn':2, 'Fin':3})
alldata['Fence'] = alldata['Fence'].map({'MnWw':0, 'GdWo':1, 'MnPrv':2, 'GdPrv':3, 'None':4})



#部分属性创造二值新属性

MasVnrType_Any = alldata.MasVnrType.replace({'BrkCmn': 1,'BrkFace': 1,'CBlock': 1,'Stone': 1,'None': 0})#replace 一样的效果
MasVnrType_Any.name='MasVnrType_Any'#series.name=   &pd.rename(columns={'A':'a', 'B':'b', 'C':'c'}, inplace = True)
SaleCondition_PriceDown = alldata.SaleCondition.replace({'Abnorml': 1,'Alloca': 1,'AdjLand': 1,'Family': 1,'Normal': 0,'Partial': 0})
SaleCondition_PriceDown.name = 'SaleCondition_PriceDown' #修改该series的列名

#map只能在SERIES上进行，replace在pd上进行
alldata=alldata.replace({'CentralAir': {'Y': 1,'N': 0}})
alldata = alldata.replace({'PavedDrive': {'Y': 1,'P': 0,'N': 0}})
newer_dwelling = alldata['MSSubClass'].map({20: 1,30: 0,40: 0,45: 0,50: 0,60: 1,70: 0,75: 0,80: 0,85: 0,90: 0,120: 1,150: 0,160: 0,180: 0,190: 0})
newer_dwelling.name= 'newer_dwelling' #修改该series的列名
alldata['MSSubClass'] = alldata['MSSubClass'].apply(str)#str()应用于series每个元素
#applymap()可作用在整个dataframe返回DF,而apply()只能作用在一列 map()只能作用在series每个元素


#目标：将某个特征一列取值多个元素变成1 将多个元素变成0
#首先创造个0值n乘1dataframe

Neighborhood_Good=pd.DataFrame(np.zeros((alldata.shape[0],1)),columns=['Neighborhood_Good'])

#Neighborhood_Good=Neighborhood_Good[alldata.Neighborhood=='NridgHt'].replace({0:1})错误，这个方法直接去除了其他值
Neighborhood_Good[alldata.Neighborhood=='NridgHt']=1
Neighborhood_Good[alldata.Neighborhood=='NridgHt'] = 1
Neighborhood_Good[alldata.Neighborhood=='Crawfor'] = 1
Neighborhood_Good[alldata.Neighborhood=='StoneBr'] = 1
Neighborhood_Good[alldata.Neighborhood=='Somerst'] = 1
Neighborhood_Good[alldata.Neighborhood=='NoRidge'] = 1
# Neighborhood_Good = (alldata['Neighborhood'].isin(['StoneBr','NoRidge','NridgHt','Timber','Somerst']))*1 #(效果没有上面好)
Neighborhood_Good.name='Neighborhood_Good'

season = (alldata['MoSold'].isin([5,6,7]))*1

season.name='season'
alldata['MoSold'] = alldata['MoSold'].apply(str)#字符化不用做归一处理，直接数值型归一化处理


#对与“质量——Qual”“条件——Cond”属性，构造新属性(一个特征两个新特征，质量差的排名，质量好的排名)
# 处理OverallQual：将该属性分成两个子属性，以5为分界线，大于5及小于5的再分别以序列
overall_poor_qu = alldata.OverallQual.copy()# Series类型
overall_poor_qu = 5 - overall_poor_qu
overall_poor_qu[overall_poor_qu<0] = 0
overall_poor_qu.name = 'overall_poor_qu'
overall_good_qu = alldata.OverallQual.copy()
overall_good_qu = overall_good_qu - 5
overall_good_qu[overall_good_qu<0] = 0
overall_good_qu.name = 'overall_good_qu'
 
# 处理OverallCond ：将该属性分成两个子属性，以5为分界线，大于5及小于5的再分别以序列
overall_poor_cond = alldata.OverallCond.copy()# Series类型
overall_poor_cond = 5 - overall_poor_cond
overall_poor_cond[overall_poor_cond<0] = 0
overall_poor_cond.name = 'overall_poor_cond'
overall_good_cond = alldata.OverallCond.copy()
overall_good_cond = overall_good_cond - 5
overall_good_cond[overall_good_cond<0] = 0
overall_good_cond.name = 'overall_good_cond'
 
# 处理ExterQual：将该属性分成两个子属性，以3为分界线，大于3及小于3的再分别以序列
exter_poor_qu = alldata.ExterQual.copy()
exter_poor_qu[exter_poor_qu<3] = 1
exter_poor_qu[exter_poor_qu>=3] = 0
exter_poor_qu.name = 'exter_poor_qu'
exter_good_qu = alldata.ExterQual.copy()
exter_good_qu[exter_good_qu<=3] = 0
exter_good_qu[exter_good_qu>3] = 1
exter_good_qu.name = 'exter_good_qu'
 
# 处理ExterCond：将该属性分成两个子属性，以3为分界线，大于3及小于3的再分别以序列
exter_poor_cond = alldata.ExterCond.copy()
exter_poor_cond[exter_poor_cond<3] = 1
exter_poor_cond[exter_poor_cond>=3] = 0
exter_poor_cond.name = 'exter_poor_cond'
exter_good_cond = alldata.ExterCond.copy()
exter_good_cond[exter_good_cond<=3] = 0
exter_good_cond[exter_good_cond>3] = 1
exter_good_cond.name = 'exter_good_cond'
 
# 处理BsmtCond：将该属性分成两个子属性，以3为分界线，大于3及小于3的再分别以序列
bsmt_poor_cond = alldata.BsmtCond.copy()
bsmt_poor_cond[bsmt_poor_cond<3] = 1
bsmt_poor_cond[bsmt_poor_cond>=3] = 0
bsmt_poor_cond.name = 'bsmt_poor_cond'
bsmt_good_cond = alldata.BsmtCond.copy()
bsmt_good_cond[bsmt_good_cond<=3] = 0
bsmt_good_cond[bsmt_good_cond>3] = 1
bsmt_good_cond.name = 'bsmt_good_cond'
 
# 处理GarageQual：将该属性分成两个子属性，以3为分界线，大于3及小于3的再分别以序列
garage_poor_qu = alldata.GarageQual.copy()
garage_poor_qu[garage_poor_qu<3] = 1
garage_poor_qu[garage_poor_qu>=3] = 0
garage_poor_qu.name = 'garage_poor_qu'
garage_good_qu = alldata.GarageQual.copy()
garage_good_qu[garage_good_qu<=3] = 0
garage_good_qu[garage_good_qu>3] = 1
garage_good_qu.name = 'garage_good_qu'
 
# 处理GarageCond：将该属性分成两个子属性，以3为分界线，大于3及小于3的再分别以序列
garage_poor_cond = alldata.GarageCond.copy()
garage_poor_cond[garage_poor_cond<3] = 1
garage_poor_cond[garage_poor_cond>=3] = 0
garage_poor_cond.name = 'garage_poor_cond'
garage_good_cond = alldata.GarageCond.copy()
garage_good_cond[garage_good_cond<=3] = 0
garage_good_cond[garage_good_cond>3] = 1
garage_good_cond.name = 'garage_good_cond'
 
# 处理KitchenQual：将该属性分成两个子属性，以3为分界线，大于3及小于3的再分别以序列
kitchen_poor_qu = alldata.KitchenQual.copy()
kitchen_poor_qu[kitchen_poor_qu<3] = 1
kitchen_poor_qu[kitchen_poor_qu>=3] = 0
kitchen_poor_qu.name = 'kitchen_poor_qu'
kitchen_good_qu = alldata.KitchenQual.copy()
kitchen_good_qu[kitchen_good_qu<=3] = 0
kitchen_good_qu[kitchen_good_qu>3] = 1
kitchen_good_qu.name = 'kitchen_good_qu'


#列向连接
qu_list = pd.concat((overall_poor_qu, overall_good_qu, overall_poor_cond, overall_good_cond, exter_poor_qu,
                     exter_good_qu, exter_poor_cond, exter_good_cond, bsmt_poor_cond, bsmt_good_cond, garage_poor_qu,
                     garage_good_qu, garage_poor_cond, garage_good_cond, kitchen_poor_qu, kitchen_good_qu), axis=1)


#对时间序列的处理,*1将bool型转化为数值型0 1

Xremoded = (alldata['YearBuilt']!=alldata['YearRemodAdd'])*1 #房子是否改建过，改建过则为True
Xrecentremoded = (alldata['YearRemodAdd']>=alldata['YrSold'])*1 #卖出去后是否改建,一般是刚卖的年刚改建
XnewHouse = (alldata['YearBuilt']>=alldata['YrSold'])*1 #是否是新房子，预售或者建好了卖
XHouseAge = 2010 - alldata['YearBuilt']#房子年纪
XTimeSinceSold = 2010 - alldata['YrSold']#房子卖出去多久了
XYearSinceRemodel = alldata['YrSold'] - alldata['YearRemodAdd']#房子改建后多久卖出去的
 
Xremoded.name='Xremoded'
Xrecentremoded.name='Xrecentremoded'
XnewHouse.name='XnewHouse'
XTimeSinceSold.name='XTimeSinceSold'
XYearSinceRemodel.name='XYearSinceRemodel'
XHouseAge.name='XHouseAge'
 
year_list = pd.concat((Xremoded,Xrecentremoded,XnewHouse,XHouseAge,XTimeSinceSold,XYearSinceRemodel),axis=1)

#pd.concat(()),函数输入多个参数要用圆括号（（）） series。isin（LIST(str))




#pandas.get_dummies（data，prefix = None，prefix_sep ='_'，dummy_na = False，columns = None，sparse = False，drop_first = False，dtype = None ）
#将分类变量转化为虚拟变量，对数据进行onehoet 编码传入的一个（或多个特征）的n个取值只有一个为1(有n个1，多了n-1列），增加特征数目
#prefix传入字符串为前缀


"""
class sklearn.svm.SVC(C=1.0, kernel=’rbf’, degree=3, gamma=’auto’, coef0=0.0, 

                       shrinking=True, probability=False, tol=0.001, cache_size=200, class_weight=None,
verbose=False, max_iter=-1, decision_function_shape=’ovr’, random_state=None)
SVC参数解释 
（1）C: 目标函数的惩罚系数C，用来平衡分类间隔margin和错分样本的，default C = 1.0；惩罚因子C决定了你有多重视离群点带来的损失，显然当所有离群点的松弛变量的和一定时，你定的C越大，对目标函数的损失也越大，此时就暗示着你非常不愿意放弃这些离群点，最极端的情况是你把C定为无限大，这样只要稍有一个点离群，目标函数的值马上变成无限大，马上让问题变成无解，这就退化成了硬间隔问题。 
（2）kernel：参数选择有RBF, Linear, Poly, Sigmoid, 默认的是"RBF"; 
（3）degree：if you choose 'Poly' in param 2, this is effective, degree决定了多项式的最高次幂； 
（4）gamma：核函数的系数('Poly', 'RBF' and 'Sigmoid'), 默认是gamma = 1 / n_features; 
（5）coef0：核函数中的独立项，'RBF' and 'Poly'有效； 
（6）probablity: 可能性估计是否使用(true or false)； 
（7）shrinking：是否进行启发式； 
（8）tol（default = 1e - 3）: svm结束标准的精度; 
（9）cache_size: 制定训练所需要的内存（以MB为单位）； 
（10）class_weight: 每个类所占据的权重，不同的类设置不同的惩罚参数C, 缺省的话自适应； 
（11）verbose: 跟多线程有关，不大明白啥意思具体； 
（12）max_iter: 最大迭代次数，default = 1， if max_iter = -1, no limited; 最大迭代次数，这个是硬限制，它的优先级要高于tol参数，不论训练的标准和精度达到要求没有，都要停止训练。默认值是-1，指没有最大次数限制; 
（13）decision_function_shape ： ‘ovo’ 一对一, ‘ovr’ 多对多  or None 无, default=None 原始的SVM只适用于二分类问题，如果要将其扩展到多类分类，就要采取一定的融合策略，这里提供了三种选择。‘ovo’ 一对一，决策所使用的返回的是（样本数，类别数*(类别数-1)/2）, ‘ovr’ 一对多，返回的是(样本数，类别数)，或者None，就是不采用任何融合策略, 默认是ovr，因为此种效果要比oro略好一点。 
（14）random_state ：用于概率估计的数据重排时的伪随机数生成器的种子。 
 ps：7,8,9一般不考虑。 
 提供的属性：
 （1）support_ : 是一个array类型，它指的是训练出的分类模型的支持向量的索引，即在所有的训练样本中，哪些样本成为了支持向量。

（2）support_vectors_: 支持向量的集合，即汇总了当前模型的所有的支持向量。

（3）n_support_ : 比如SVC将数据集分成了4类，该属性表示了每一类的支持向量的个数。

（4）dual_coef_ :array, shape = [n_class-1, n_SV]对偶系数

       支持向量在决策函数中的系数，在多分类问题中，这个会有所不同。

（5）coef_ : array,shape = [n_class-1, n_features]

       该参数仅在线性核时才有效，指的是每一个属性被分配的权值。

（6）intercept_ :array, shape = [n_class * (n_class-1) / 2]决策函数中的常数项bias。和coef_共同构成决策函数的参数值。
 
 
 
 
 """
from sklearn.svm import SVC
svm=SVC(C=100,kernel='rbf',gamma=0.0001)
pc=pd.DataFrame(np.zeros((train['SalePrice'].shape[0],1)))
pc[:]='pc1'
pc[train['SalePrice']>=150000]='pc2'
pc[train['SalePrice']>=220000]='pc3'

columns_for_pc = ['Exterior1st', 'Exterior2nd', 'RoofMatl', 'Condition1', 'Condition2', 'BldgType']
X_t=pd.get_dummies(train.ix[:,columns_for_pc],sparse=True)#稀疏矩阵
svm.fit(X_t,pc)#训练

p = train.SalePrice/100000



price_category = pd.DataFrame(np.zeros((alldata.shape[0],1)),columns=['price_category'])

X_t = pd.get_dummies(alldata.loc[:, columns_for_pc], sparse=True)

pc_pred = svm.predict(X_t) # 预测测试集上这一新特征所取的值

price_category[pc_pred=='pc1'] = 0
price_category[pc_pred=='pc2'] = 1
price_category[pc_pred=='pc3'] = 2
price_category.name='price_category'

#连续数据离散化（年份离散化）
yearmap=pd.concat((pd.Series('YearGroup'+str(i+1),index=range(1871+i*20,1891+i*20))for i in range(0,7)))
#str(i+1)将i+1转化为字符，‘i+1’不会运算的
alldata.YearBuilt = alldata.YearBuilt.map(year_map)
alldata.YearRemodAdd = alldata.YearRemodAdd.map(year_map)



#按比例缩放
#获取数值型数据  pd.dtypes返回一个series 每列类型
numeric_index=alldata.dtypes[alldata.dtypes!='object'].index
#numeric_index=alldata.ix[:,alldata.dtypes!='object'].index 运算更大
t=alldata.ix[:,numeric_index].quantile(.75)
t = alldata[numeric_index].quantile(.75)#直接输入index类型可以多列输入
use_75_scater = t[t != 0].index
alldata[use_75_scater] = alldata[use_75_scater]/alldata[use_75_scater].quantile(.75)



#标准化数据使符合正态分布
'''
Box-Cox变换是Box和Cox在1964年提出的一种广义幂变换方法，是统计建模中常用的一种数据变换，
用于连续的响应变量不满足正态分布的情况。Box-Cox变换之后，可以一定程度上减小不可观测的误差和预测变量的相关性。
Box-Cox变换的主要特点是引入一个参数，通过数据本身估计该参数进而确定应采取的数据变换形式，
Box-Cox变换可以明显地改善数据的正态性、对称性和方差相等性，对许多实际数据都是行之有效的.

Box-Cox变换的目的是为了让数据满足线性模型的基本假定，即线性、正态性及方差齐性，
然而经Box-Cox变换后数据是否同时满足了以上假定，仍需要考察验证 [2]  。

scipy.special.boxcox1p （x，lmbda ） lmbda 是参数


scipy.special.boxcox1p （x，lmbda ） lmbda 是参数
boxcox1p计算的Box-Cox变换是：

y =（（1 + x）** lmbda  -  1）/ lmbda如果lmbda！= 0
    log（1 + x）如果lmbda == 0
np.loglp()......log(x+1)

'''
from scipy.special import boxcox1p#引入统计模块的BOX-COX变换（计量经济学）

t = ['LotFrontage', 'LotArea', 'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF',
     '1stFlrSF', '2ndFlrSF', 'LowQualFinSF', 'GrLivArea', 'GarageArea', 'WoodDeckSF', 'OpenPorchSF',
     'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea', 'MiscVal']
# alldata.loc[:, t] = np.log1p(alldata.loc[:, t])
train["SalePrice"] = np.log1p(train["SalePrice"]) # 对于SalePrice 采用log1p较好---np.expm1(clf1.predict(X_test))
#平滑数据使其更接近正太函数

lam=0.15
for i in t:
    alldata[i]=boxcox1p(alldata[t],lam)#对连续变量用boxcox1p



# 将标称型变量二值化

X = pd.get_dummies(alldata)
X = X.drop('Condition2_PosN', axis=1)#重复特征
X = X.drop('MSZoning_C (all)', axis=1)#
X = X.drop('MSSubClass_160', axis=1)


X= pd.concat((X, newer_dwelling, season, year_list ,qu_list,MasVnrType_Any, price_category,SaleCondition_PriceDown,Neighborhood_Good), axis=1)#SaleCondition_PriceDown,Neighborhood_Good已经加入
X.to_csv('new.csv')
'''
a=pd.DataFrame(pd.Series(range(0,2917)))#输入列名复杂还不如如此,前排当作只能传入series和dataframe和array，columns=必须是列表
a.name='a'#series 改name再concat name自动为那列列名 ，dataframe一列改name还是默认原来的值 例如默认0
a
'''    





'''
product(iter1,iter2, ... iterN, [repeat=1]);创建一个迭代器，生成表示item1，item2等中的项目的笛卡尔积的元组，repeat是一个关键字参数，指定重复生成序列的次数

    # product('ABCD', 'xy') --> Ax Ay Bx By Cx Cy Dx Dy
    # product(range(2), repeat=3) --> 000 001 010 011 100 101 110 111 ,重复三次


chain()
      与其名称意义一样，给它一个列表如 lists/tuples/iterables，链接在一起；返回iterables对象。

 

letters = ['a', 'b', 'c', 'd', 'e', 'f']
booleans = [1, 0, 1, 0, 0, 1]
 
    print(list(itertools.chain(letters,booleans)))
#     ['a', 'b', 'c', 'd', 'e', 'f', 1, 0, 1, 0, 0, 1]
'''
from itertools import product,chain
def poly(x):
    a=['LotArea', 'TotalBsmtSF', 'GrLivArea', 'GarageArea', 'BsmtUnfSF']
    t=chain(qu_list.axes[1].get_values(),year_list.axes[1].get_values(),ordinalList,['MasVnrType_Any'])
#df或者Series或者index类型方法get_values()或者属性values ，axes[]默认为输出df的index  为1的话输出columns   
    for a,t in product(a,t):
        x=alldata.ix[:,[a,t]].prod(1)#prod()连乘   prod(1)两列两两连乘,生成series,    
        x.name=a+'_'+t
        yield x
# 带有 yield 的函数在 Python 中被称之为 generator（生成器）
# 一个带有 yield 的函数就是一个 generator，它和普通函数不同，生成一个 generator 看起来像函数调用，
# 但不会执行任何函数代码，直到对其调用 next()（在 for 循环中会自动调用 next()）才开始执行。
# 虽然执行流程仍按函数的流程执行，但每执行到一个 yield 语句就会中断，并返回一个迭代值，
# 下次执行时从 yield 的下一个语句继续执行。看起来就好像一个函数在正常执行的过程中被 yield 中断了数次，
# 每次中断都会通过 yield 返回当前的迭代值。
XP = pd.concat(poly(X), axis=1) # (2917, 155)
X = pd.concat((X, XP), axis=1) # (2917, 466)
X_train = X[:train.shape[0]]
X_test = X[train.shape[0]:]
print(X_train.shape)#(1458, 460)
Y= train.SalePrice
train_now = pd.concat([X_train,Y], axis=1)
test_now = X_test


train_now.to_csv('train_afterchange.csv')
test_now.to_csv('test_afterchange.csv')





#模型构建
#导入需要的包
import pandas as pd
import numpy as np

import matplotlib.pyplot as plt
import seaborn as sns
from  sklearn.model_selection import cross_val_score
'''
n_splits：表示划分几等份

shuffle：在每次划分时，是否进行洗牌

①若为Falses时，其效果等同于random_state等于整数，每次划分的结果相同（所以KFOLD等方法是无放回因为默认）

②若为True时，每次划分的结果都不一样，表示经过洗牌，随机取样的

random_state：随机种子数

属性：

①get_n_splits(X=None, y=None, groups=None)：获取参数n_splits的值

②split(X, y=None, groups=None)：将数据集划分成训练集和测试集，返回索引生成器

通过一个不能均等划分的栗子，设置不同参数值，观察其结果

①设置shuffle=False，运行两次，发现两次结果相同

frome sklearn.model_selecction import 
1.train_test_split(X,目标变量y，test_size=：如果是浮点数，在0-1之间，表示样本占比；
如果是整数的话就是样本的数量，train_size，shuffle=是否在拆分之前对数据进行洗牌，random_state=0）
    1仅仅是划分数据集，且是整个数据集按比例划分，可以打乱划分也可以不打乱，即无放回，
并且y也是划分，y是其他的输入）
# random_state：设置随机种子的，为了每次迭代都有相同的训练集顺序（如果有的话）

2.cross_val_score返回的是评测效果
 sklearn.model_selection.cross_val_score(estimator, X, y=None, groups=None, scoring=None, cv=None）
  
   其中，score 默认是以 scoring=’f1_macro’进行评测的这需要from　sklearn import metrics 
,通过在cross_val_score 指定参数来设定评测标准； 当cv 指定为int 类型时，默认使用KFold 
或StratifiedKFold 进行数据集打乱。
   
  estimator 拟合数据的学习器（必须有fit方法可以没有transform方法）
  y目标变量
  cv迭代次数（当cv 指定为int 类型时，默认使用KFold 或StratifiedKFold 进行数据集划分）
  当然也可以通过指定cv而不用默认的）
      
    例如cv=ShuffleSplit(n_splits=3, test_size=.3, random_state=0)#随机打乱最好加上
random_state=0，不然系统默认为时间，下次随机抽取会不一样）因为规定了size所以是迭代次数
 


  在cross_val_score 中同样可使用pipeline 进行流水线操作
  我们要用 Pipeline 对训练集和测试集进行如下操作：

  先用 StandardScaler 对数据集每一列做标准化处理，（是 transformer）
再用 PCA 将原始的 30 维度特征压缩的 2 维度，（是 transformer）
最后再用模型 LogisticRegression。（是 Estimator）
调用 Pipeline 时，输入由元组构成的列表，每个元组第一个值为变量名，元组第二个元素是 
sklearn 中的 transformer 或 Estimator。
  
   注意中间每一步是 transformer，即它们必须包含 fit 和 transform 方法，或者 fit_transform。 
最后一步是一个 Estimator，即最后一步模型要有 fit 方法，可以没有 transform 方法。
   
  然后用 Pipeline.fit对训练集进行训练，pipe_lr.fit(X_train, y_train) #sklearn机器学习库
再直接用 Pipeline.score 对测试集进行预测并评分 pipe_lr.score(X_test, y_test)
   from sklearn.prepocessing import StandardScaler
   from sklearn.decompositon import PCA  #decomposition分解
   frome sklearn.linear_model import LogisiticRegression

   from sklearn.pipeline import Pipeline
   #构建Pipeline
   pipe_lr=Pipeline([('sc',StandardScaler()),
                     ('pca',PCA(n_components=2)),
                     ('clf',LogisticRegression(random_state=1))])
    pipe_lr.fit(X_train,y_train)#拟合数据训练数据
    print('测试准确性：%.3f'%pipe_lr.score(X_test,y_test))#预测数据并评测F1
   
make_pipeline函数是Pipeline类的简单实现，只需传入每个step的类实例即可，不需自己命名，
自动将类的小写设为该step的名。


In [50]: make_pipeline(StandardScaler(),GaussianNB())
Out[50]: Pipeline(steps=[('standardscaler', StandardScaler(copy=True, with_mean=
True, with_std=True)), ('gaussiannb', GaussianNB(priors=None))])
 
In [51]: p=make_pipeline(StandardScaler(),GaussianNB())
 
In [52]: p.steps
Out[52]:
[('standardscaler', StandardScaler(copy=True, with_mean=True, with_std=True)),
 ('gaussiannb', GaussianNB(priors=None))]


数据预处理模块  preprocessing
preprocessing.StandardScaler(copy=True, with_mean=True,with_std=True)：
标准正态分布化的类





3.cross_val_predict

 cross_val_predict 与cross_val_score 很相像，不过不同于返回的是评测效果，cross_val_predict 
返回的是estimator 的分类结果（或回归值），这个对于后期模型的改善很重要，可以通过该预测输出
对比实际目标值，准确定位到预测出错的地方，为我们参数优化及问题排查十分的重要。
  一样当cv 指定为int 类型时，默认使用KFold 或StratifiedKFold 进行数据集划分）
  当然也可以通过指定cv而不用默认的）


4.KFold
n_splits : 默认3，最小为2；K折验证的K值

shuffle : 默认False;shuffle会对数据产生随机搅动(洗牌)

random_state :默认None，随机种子
也仅仅是划分划分数据集，这个是无放回划分
In [33]: from sklearn.model_selection import KFold

In [34]: X = ['a','b','c','d']

In [35]: kf = KFold(n_splits=2)

In [36]: for train, test in kf.split(X):#属性spilt②split(X, y=None, groups=None)：将数据集划分成
#训练集和测试集，返回索引生成器  get_n_splits(X=None, y=None, groups=None)：获取参数n_splits的值

    ...:     print train, test
    ...:     print np.array(X)[train], np.array(X)[test]
    ...:     print '\n'
    ...:     
[2 3] [0 1]
['c' 'd'] ['a' 'b']


[0 1] [2 3]
['a' 'b'] ['c' 'd']#可以看出来返回的是index  。kf = KFold(n_splits=2) 知道我们将数据集分成两部分，可
以定义训练集测试集。n_splits表示分成几部分

from sklearn.model_selection import KFold
X = ['a','b','c','d']
kf = KFold(n_splits=3)
for train, test in kf.split(X):
     print  (train, test)
     print (np.array(X)[train], np.array(X)[test])
     print ('\n')
    
    
    
    
5.LeaveOneOut留一法
In [37]: from sklearn.model_selection import LeaveOneOut

In [38]: X = [1,2,3,4]

In [39]: loo = LeaveOneOut()

In [41]: for train, test in loo.split(X):
    ...:     print train, test
    ...:     
[1 2 3] [0]
[0 2 3] [1]
[0 1 3] [2]
[0 1 2] [3]


#使用KFold实现LeaveOneOtut
In [42]: kf = KFold(n_splits=len(X))

In [43]: for train, test in kf.split(X):
    ...:     print train, test
    ...:     
[1 2 3] [0]
[0 2 3] [1]
[0 1 3] [2]
[0 1 2] [3]
#一样是index ，n_splits=len(X)分成四部分，选一个剩余三个预测弄它

6.LeavePOut
In [44]: from sklearn.model_selection import LeavePOut

In [45]: X = np.ones(4)

In [46]: lpo = LeavePOut(p=2)

In [47]: for train, test in lpo.split(X):
    ...:     print train, test
    ...:     
[2 3] [0 1]
[1 3] [0 2]
[1 2] [0 3]
[0 3] [1 2]
[0 2] [1 3]
[0 1] [2 3]
#分成四部

7.ShuffleSplit（是有放回的抽样，只能说经过一个足够大的抽样次数后，保证测试集出现了完成的数据集的倍数。）

from sklearn.model_selection import ShuffleSplit
X=np.arange(4)
ss=ShuffleSplit(n_splits=4,test_size=.25,random_state=0)#不加random_state=0系统时间不一样每次结果不一样
for train_index,test_index in ss.split(X):
    print(train_index,test_index)

n_splits指迭代次数，因为有测试及训练集大小 所以是迭代次数   (不管是迭代次数还是分成几部分都是n个结果)
    
    
8.StratifiedKFold 
这个就比较好玩了，通过指定分组，对测试集进行无放回抽样。

In [52]: from sklearn.model_selection import StratifiedKFold

In [53]: X = np.ones(10)

In [54]: y = [0,0,0,0,1,1,1,1,1,1]

In [55]: skf = StratifiedKFold(n_splits=3)#分成三部分

In [56]: for train, test in skf.split(X,y):
    ...:     print train, test
    ...:     
[2 3 6 7 8 9] [0 1 4 5]
[0 1 3 4 5 8 9] [2 6 7]
[0 1 2 4 5 6 7] [3 8 9]

13.TimeSeriesSplit
针对时间序列的处理，防止未来数据的使用，分割时是将数据进行从前到后切割（这个说法其实不太恰当，因为切割是延续性的。。）

In [81]: from sklearn.model_selection import TimeSeriesSplit

In [82]: X = np.array([[1,2],[3,4],[1,2],[3,4],[1,2],[3,4]])

In [83]: tscv = TimeSeriesSplit(n_splits=3)

In [84]: for train, test in tscv.split(X):
    ...:     print train, test
    ...:     
[0 1 2] [3]
[0 1 2 3] [4]
[0 1 2 3 4] [5]
'''
from sklearn.metrics import make_scorer,mean_squared_error
from sklearn.linear_model import Ridge,RidgeCV,ElasticNet,LassoCV,LassoLarsCV
#lasticNet回归，目标函数为：也就是岭回归和Lasso回归的组合。
from sklearn.model_selection import cross_val_score
from operator import itemgetter
#  from operator import itemgetter或 import operator （调用时需要用itemgetter.operator)
'''
a = [1,2,3] 
>>> b=operator.itemgetter(1)      //定义函数b，获取对象的第1个域的值
>>> b(a) 
2 
>>> b=operator.itemgetter(1,0)   //定义函数b，获取对象的第1个域和第0个的值
>>> b(a) 
(2, 1) 
  要注意，operator.itemgetter函数获取的不是值，而是定义了一个函数，通过该函数作用到对象上才能获取值。
a = [1,2,3] 
b = [[1,2,3],[4,5,6],[7,8,9]] 
get_1 = itemgetter(1)
get_1(a)  >>> 2
get_1(b)  >>> [4,5,6]
'''
import itertools
#引入迭代器库
#忽略警告提醒
import warnings
warnings.filterwarnings('ignore')
#%matplotlib inline
#可以在Ipython编译器里直接使用，功能是可以内嵌绘图，并且可以省略掉plt.show()这一步
#是在使用jupyter notebook 或者 jupyter qtconsole的时候，才会经常用而%matplotlib具体作用是当你
#调用matplotlib.pyplot的绘图函数plot()进行绘图的时候，或者生成一个figure画布的时候，可以直接
#在你的python console里面生成图像

pd.set_option('display.float_format',lambda x:'{:.3f}'.format(x))#浮点数据输出禁运科学计数法
#'{:.2f}'.format(x)
'''
#填充与对齐
32 print '{:>8}'.format('189')
33 #     189
34 print '{:0>8}'.format('189')
35 #00000189
36 print '{:a>8}'.format('189')
37 #aaaaa189
38 
39 #精度与类型f
40 #保留两位小数
41 print '{:.2f}'.format(321.33345)
42 #321.33
'''
import xgboost as xgb
train=pd.read_csv('train_afterchange.csv')
test=pd.read_csv('test_afterchange.csv')

alldata = pd.concat((train.iloc[:,1:-1], test.iloc[:,1:]), ignore_index=True)
#-1就是最后一列的标签  ignore_index=True索引没有意义一开始一定要注意这个
alldata.shape
X_train=train.iloc[:,1:-1]
y=train.iloc[:,-1]
X_test=test.iloc[:,1:]
print (X_train.shape,y.shape,X_test.shape)

def rmse_cv(model):
    rmse=np.sqrt(-cross_val_score(model,X_train,y,scoring='neg_mean_squared_error',cv=5))#cv=5 五折
#KFOLD分成五份 ，五组数据  不加负号报错难道是neg的原因- -
    return(rmse)

#Lasso model
cl=LassoCV(alphas=np.logspace(-2,-7,50))#通过LassoCV内置函数挑选 alpha
#alpha=np.logspace(-3,2,50)#logspace创造等比序列，本例从10^-3到10^2其中的50个数
#无，使用默认的3倍交叉验证，默认KFOLD 三部分，可以自定义。
cl.fit(X_train,y)#classcv属性predict，拟合数据，训练模型，先用全部训练数据挑选出最佳的模型代入交叉运算
cl.alpha_#查看寻找的参数
lasso_preds=np.expm1(cl.predict(X_test))#预测一波值# exp(x) - 1  <---->log1p(x)==log(1+x)

score1=rmse_cv(cl)
print('\nLasso score:{:4f}({:.4f})\n'.format(score1.mean(),score1.std()))#不加括号连在一起
#加\n让输出不在一起美观  std()计算标准差

#ELASTIC NET

#lasso回归惩罚系数lambda1和岭回归Ridge lambda2的指定是通过l1_ratio和alpha来完成（可以设置成网格自动搜索）
clf2 = ElasticNet(alpha=0.0005, l1_ratio=0.9)#建立模型
#ElasticNetCV 可以通过交叉验证来用来设置参数 alpha () 和 l1_ratio ，
#ElasticNetCV才有clf2.alpha_  只有交叉验证才有
clf2.fit(X_train, y)#通过fit拟合数据训练模型

#通过属性alpha_查看寻找的参数
elas_preds = np.expm1(clf2.predict(X_test))#通过属性predict输出预测值
 
score2 = rmse_cv(clf2)
print("\nElasticNet score: {:.4f} ({:.4f})\n".format(score2.mean(), score2.std()))

'''
from sklearn.linear_model import ElasticNetCV #sklearn.model_selection 交叉验证选择模型
clf3=ElasticNetCV(alphas=np.logspace(-7,-2,50),l1_ratio=np.logspace(2,-3,50))#CV模型参数alphas多了s
clf3.fit(X_train,y)
clf3.alpha_
elasCV_pred=clf3.predict(X_test)
score4=rmse_cv(clf3)
elasCV_pred=expm(elasCV_pred)
print('\nElasticNetCV score:{:.4f}({:.4f})\n'.format(score4.mean(),score4.std()))#把format包含进去
'''
#XGBOOST     
clf3=xgb.XGBRegressor(colsample_bytree=0.4,  
                 gamma=0.045,  
                 learning_rate=0.07,  
                 max_depth=20,  
                 min_child_weight=1.5,  
                 n_estimators=300,  
                 reg_alpha=0.65,  
                 reg_lambda=0.45,  
                 subsample=0.95)  
'''
1.booster [default=gbtree]：选择基分类器，gbtree: tree-based models/gblinear: linear models 
2.silent [default=0]:设置成1则没有运行信息输出，最好是设置为0. 
3.nthread [default to maximum number of threads available if not set]：线程数

Booster Parameters（模型参数） 
1.learning_rate [default=0.3]:shrinkage参数，用于更新叶子节点权重时，乘以该系数，避免步长过大。参数值越大，越可能无法收敛。把学习率 eta 设置的小一些，小学习率可以使得后面的学习更加仔细。 
2.min_child_weight [default=1]:这个参数默认是 1，是每个叶子里面 h 的和至少是多少，对正负样本不均衡时的 0-1 分类而言，假设 h 在 0.01 附近，min_child_weight 为 1 意味着叶子节点中最少需要包含 100 个样本。这个参数非常影响结果，控制叶子节点中二阶导的和的最小值，该参数值越小，越容易 overfitting。 
3.max_depth [default=6]: 每颗树的最大深度，树高越深，越容易过拟合。 
4.max_leaf_nodes:最大叶结点数，与max_depth作用有点重合。 
5.gamma [default=0]：后剪枝时，用于控制是否后剪枝的参数。 
6.max_delta_step [default=0]：这个参数在更新步骤中起作用，如果取0表示没有约束，如果取正值则使得更新步骤更加保守。可以防止做太大的更新步子，使更新更加平缓。 
7.subsample [default=1]：样本随机采样，较低的值使得算法更加保守，防止过拟合，但是太小的值也会造成欠拟合。 
8.colsample_bytree [default=1]：列采样，对每棵树的生成用的特征进行列采样.一般设置为： 0.5-1 
9.reg_lambda [default=1]：控制模型复杂度的权重值的L2正则化项参数，参数越大，模型越不容易过拟合。 
10.reg_alpha [default=0]:控制模型复杂程度的权重值的 L1 正则项参数，参数值越大，模型越不容易过拟合。 
11.scale_pos_weight [default=1]：如果取值大于0的话，在类别样本不平衡的情况下有助于快速收敛。

Learning Task Parameters（学习任务参数） 
1.objective [default=reg:linear]：定义最小化损失函数类型，常用参数： 
binary:logistic –logistic regression for binary classification, returns predicted probability (not class) 
multi:softmax –multiclass classification using the softmax objective, returns predicted class (not probabilities) 
you also need to set an additional num_class (number of classes) parameter defining the number of unique classes 
multi:softprob –same as softmax, but returns predicted probability of each data point belonging to each class. 
2.eval_metric [ default according to objective ]： 
The metric to be used for validation data. 
The default values are rmse for regression and error for classification. 
Typical values are: 
rmse – root mean square error 
mae – mean absolute error 
logloss – negative log-likelihood 
error – Binary classification error rate (0.5 threshold) 
merror – Multiclass classification error rate 
mlogloss – Multiclass logloss 
auc: Area under the curve 
3.seed [default=0]： 
The random number seed. 随机种子，用于产生可复现的结果 
Can be used for generating reproducible results and also for parameter tuning.

注意: python sklearn style参数名会有所变化 
eta –> learning_rate 
lambda –> reg_lambda 
alpha –> reg_alpha
'''
clf3.fit(X_train, y.values)  
xgb_preds = np.expm1(clf3.predict(X_test))  #log（x+1）反运算
  
  
score3 = rmse_cv(clf3)  
print("\nxgb score: {:.4f} ({:.4f})\n".format(score3.mean(), score3.std()))


final_result = 0.7*lasso_preds + 0.15*xgb_preds+0.15*elas_preds #0.11327
 
solution = pd.DataFrame({"id":test.index+1461, "SalePrice":final_result}, columns=['id', 'SalePrice'])
solution.to_csv("result011621.csv", index = False)  #如果不交代columns=它列会按大小排序？反正不按输入顺序排序
#普通的Ensemble(集成学习)主要有Bagging,Boosting以及Stacking等
    
    

    
    
    
    
    
    
    
    
    
    
    
    
