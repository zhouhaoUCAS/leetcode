# -*- coding: utf-8 -*-
"""
Created on Wed Aug  1 11:13:58 2018

@author: zh




交叉函数自带寻优
from sklearn.model_selection import
"""
import pandas as pd
import numpy as np

from sklearn.metrics import make_scorer,mean_squared_error
from sklearn.linear_model import Ridge,RidgeCV,ElasticNet,LassoCV,LassoLarsCV
#lasticNet回归，目标函数为：也就是岭回归和Lasso回归的组合。
from sklearn.model_selection import cross_val_score
from operator import itemgetter

import itertools
#引入迭代器库
#忽略警告提醒
import warnings
warnings.filterwarnings('ignore')
#%matplotlib inline
#可以在Ipython编译器里直接使用，功能是可以内嵌绘图，并且可以省略掉plt.show()这一步
pd.set_option('display.float_format',lambda x:'{:.3f}'.format(x))#浮点数据输出禁运科学计数法
import xgboost as xgb

import os
os.chdir(r'F:\项目\houce price\House Prices Advanced Regression Techniques')#改变工作区间

train=pd.read_csv('train_afterchange.csv')
test=pd.read_csv('test_afterchange.csv')

alldata = pd.concat((train.iloc[:,1:-1], test.iloc[:,1:]), ignore_index=True)
#-1就是最后一列的标签  ignore_index=True索引没有意义一开始一定要注意这个
alldata.shape
X_train=train.iloc[:,1:-1]
y=train.iloc[:,-1]
X_test=test.iloc[:,1:]
print (X_train.shape,y.shape,X_test.shape)

def rmse_cv(model):
    rmse=np.sqrt(-cross_val_score(model,X_train,y,scoring='neg_mean_squared_error',cv=5))#cv=5 五折
#KFOLD分成五份 ，五组数据  不加负号报错难道是neg的原因- -
    return(rmse)

#Lasso model
cl=LassoCV(alphas=[1,0.1,0.001,0.0005,0.0003,0.0002,5e-4])#通过LassoCV内置函数挑选 alpha
#alpha=np.logspace(-3,2,50)#logspace创造等比序列，本例从10^-3到10^2其中的50个数
#必须cl=LassoCV(alphas=np.logspace(-3,2,50))如果cl=LassoCV(alphas)会报错
cl.fit(X_train,y)#classcv属性predict，拟合数据，训练模型
cl.alpha_#查看寻找的参数
lasso_preds=np.expm1(cl.predict(X_test))#预测一波值# exp(x) - 1  <---->log1p(x)==log(1+x)

score1=rmse_cv(cl)
print('\nLasso score:{:4f}({:.4f})\n'.format(score1.mean(),score1.std()))#不加括号连在一起
#加\n让输出不在一起美观

'''
但是我们想看参数和错误关系
alphas=np.logspace(-7,-2,50)#np.linspace() 生成(start,stop)区间指定元素个数num的list等差数列，调参
#alphas=np.logspace(-3,2,num=4, endpoint=False，base=2.0)num制定个数，endpoint不要尾点正常两端都有，base将10变成2，0.001变成八分之一
alphas
test_score=[]
for alpha in alphas:
   clf=LassoCV(alpha)
   scores=np.sqrt(-cross_val_score(clf,X_train,y,cv=5,scoring='neg_mean_squared_error'))
   test_score.append(np.mean(scores))#cross_val_score返回的是迭代五个值所以取均值，每个参数交叉检验均值误差比较
   
import  matplotlib.pyplot as plt
import seaborn as sns
plt.figure(figsize=(12,9))
plt.plot(alphas,test_score)
plt.xlim(0,0.0002)
plt.ylim(0,0.2)
plt.xlabel('alpha')
plt.ylabel('score')
plt.title('alpha vs error')
plt.show()
'''
'''
from sklearn.ensemble import BaggingRegressor
from sklearn.ensemble import BaggingRegressor

Bagging通过引入随机化增大每个估计器之间的差异。



参数介绍：

　　　　base_estimator：Object or None。None代表默认是DecisionTree，Object可以指定基估计器（base estimator）。

　　　　n_estimators：int, optional (default=10) 。   要集成的基估计器的个数。

　　　　max_samples： int or float, optional (default=1.0)。决定从x_train抽取去训练基估计器的样本数量。int 代表抽取数量，float代表抽取比例

　　　　max_features : int or float, optional (default=1.0)。决定从x_train抽取去训练基估计器的特征数量。int 代表抽取数量，float代表抽取比例

　　　　bootstrap : boolean, optional (default=True) 决定样本子集的抽样方式（有放回和不放回）

　　　　bootstrap_features : boolean, optional (default=False)决定特征子集的抽样方式（有放回和不放回）

　　　　oob_score : bool 决定是否使用包外估计（out of bag estimate）泛化误差

　　　　warm_start : bool, optional (default=False) true代表

　　　　n_jobs : int, optional (default=1) 

　　　　random_state : int, RandomState instance or None, optional (default=None)。如果int，random_state是随机数生成器使用的种子; 如果RandomState的实例，random_state是随机数生成器; 如果None，则随机数生成器是由np.random使用的RandomState实例。

　　　　verbose : int, optional (default=0) 

属性介绍：

　　　　estimators_ : list of estimators。The collection of fitted sub-estimators.

　　　　estimators_samples_ : list of arrays

　　　　estimators_features_ : list of arrays

　　　　oob_score_ : float，使用包外估计这个训练数据集的得分。

　　　　oob_prediction_ : array of shape = [n_samples]。在训练集上用out-of-bag估计计算的预测。 如果n_estimator很小，则可能在抽样过程中数据点不会被忽略。 在这种情况下，oob_prediction_可能包含NaN。





Bagging
下面把alpha=0.5的岭回归模型作为基分类器，看一下Bagging的效果：

from sklearn.ensemble import BaggingRegressor
1
params = [1, 10, 15, 20, 25, 30, 40]
test_scores = list()
for param in params:
    clf = BaggingRegressor(n_estimators=param, base_estimator=Ridge(15))
    test_score = np.sqrt(-cross_val_score(clf, X_train, y_train, cv=10, scoring='neg_mean_squared_error'))
    test_scores.append(np.mean(test_score))
    
    
    
    
    
    
    
    
    AdaBoost
最后看一下AdaBoost的效果，基分类器仍然是Ridge(15)

from sklearn.ensemble import AdaBoostRegressor
1
params = [10, 15, 20, 25, 30, 35, 40, 45, 50]
test_scores = list()
for param in params:
    clf = AdaBoostRegressor(n_estimators=param, base_estimator=Ridge(15))
    test_score = np.sqrt(-cross_val_score(clf, X_train, y_train, cv=10, scoring='neg_mean_squared_error'))
    test_scores.append(np.mean(test_score))
1
2
3
4
5
6
plt.plot(params, test_scores)
plt.title("Params vs CV Error")

'''

交叉验证自带参数寻优


Validation curves验证曲线：前面主要讲的是模型的评估，主要思想就是采用多次数据分割的思想进行交叉验证，用每次分割的得分均值评估模型。这里主要讲通过可视化了解模型的训练过程，判断模型是否过拟合，并进行参数选择。

1，validation_curve验证曲线方法：这个方法是用来测试模型不同参数（不同模型）的得分情况。重要参数为：param_name代表要控制的参数；param_range代表参数的取值列表。其他参数都一样。返回训练和测试集上的得分。 
（根据模型在训练集合测试集上，不同参数取值的结果，判断模型是否过拟合，并进行参数选择；每次针对一个参数）

from sklearn.model_selection import validation_curve   #可视化学习的整个过程
from sklearn.datasets import load_digits
from sklearn.svm import SVC
import matplotlib.pyplot as plt
import numpy as np

digits=load_digits()
X=digits.data
y=digits.target
gamma_range=np.logspace(-6,-2.3,5)#从-6到-2.3取5个点
train_loss,test_loss=validation_curve(
    SVC(),X,y,param_name="gamma",param_range=gamma_range,cv=10,scoring="neg_mean_squared_error")
#param_name指定优化参数，param_range指定优化参数取值范围
train_loss_mean= (-1)*np.mean(train_loss,axis=1)#模型的评估非交叉检验
test_loss_mean= (-1)*np.mean(test_loss,axis=1)

plt.plot(gamma_range,train_loss_mean,"o-",color="r",label="Training")
plt.plot(gamma_range,test_loss_mean,"o-",color="g",label="Cross-validation")

plt.xlabel("gamma")
plt.ylabel("Loss")
plt.legend(loc="best")
plt.show()
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22#learning_curve相比于validation_curve没有param_name="gamma",param_range=gamma_range所以多返回了训练集大小
2，learning_curve学习曲线方法：这个方法显示了对于不同数量的训练样本的模型的验证和训练评分。 这是一个工具，可以找出我们从添加更多的训练数据中受益多少，以及估计器是否因方差错误或偏差错误而受到更多的影响。 如果验证分数和训练分数都随着训练集规模的增加而收敛到一个太低的值，那么我们就不会从更多的训练数据中受益。 在下面的情节中，你可以看到一个例子：朴素贝叶斯大致收敛到一个低分。 
方法重要参数：train_sizes代表每次选择进行训练的数据大小。 
方法返回：训练数据大小列表，训练集，测试集上的得分列表

from sklearn.model_selection import learning_curve   #可视化学习的整个过程
from sklearn.datasets import load_digits
from sklearn.svm import SVC
import matplotlib.pyplot as plt
import numpy as np

digits=load_digits()
X=digits.data
y=digits.target

#交叉验证测试
train_sizes,train_loss,test_loss = learning_curve(SVC(gamma=0.1),X,y,cv=10,scoring='neg_mean_squared_error',train_sizes=[0.1,0.25,0.5,0.75,1])   #记录的点是学习过程中的10%，25%等等的点
train_loss_mean = -1 * np.mean(train_loss,axis=1)
test_loss_mean = -1 * np.mean(test_loss,axis=1)

#可视化展示
plt.subplot(1,2,1)
plt.plot(train_sizes,train_loss_mean,'o-',color='r',label='train')
plt.plot(train_sizes,test_loss_mean,'o-',color='g',label='cross_validation')

plt.xlabel("Training examples")
plt.ylabel("Loss")
plt.legend(loc="best")

#交叉验证测试
train_sizes,train_loss,test_loss = learning_curve(SVC(gamma=0.001),X,y,cv=10,scoring='neg_mean_squared_error',train_sizes=[0.1,0.25,0.5,0.75,1])   #记录的点是学习过程中的10%，25%等等的点
train_loss_mean = 1 * np.mean(train_loss,axis=1)
test_loss_mean = 1 * np.mean(test_loss,axis=1)

#可视化展示
plt.subplot(1,2,2)
plt.plot(train_sizes,train_loss_mean,'o-',color='r',label='train')
plt.plot(train_sizes,test_loss_mean,'o-',color='g',label='cross_validation')

plt.xlabel("Training examples")
plt.ylabel("Loss")
plt.legend(loc="best")
plt.show()
























